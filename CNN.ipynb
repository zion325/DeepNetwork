{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b494bdd",
   "metadata": {},
   "source": [
    "# CNN神经网络训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b6975",
   "metadata": {},
   "source": [
    "## CNN神经网络类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8174ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class CNN:\n",
    "    def __init__(self,input_size,conlayer,poolingLayer,fclayer):\n",
    "        '''\n",
    "        input_size: 数据大小，一个元组，例如图片（28，28，1）\n",
    "        conlayer: 卷积层结构，一个列表，包含每个卷积层的参数元组 (num_filters, filter_size, padding, stride)\n",
    "        poolingLayer: 池化层结构(最大池化层)，一个整数，池化层的池化大小\n",
    "        fclayer: 全连接层，一个元组，(num_units, num_classes)\n",
    "        '''\n",
    "        self.input_size = input_size\n",
    "        self.conlayer = conlayer\n",
    "        self.poolingLayer = poolingLayer\n",
    "        self.fclayer = fclayer\n",
    "        self.learn_rate = 0.1\n",
    "        \n",
    "        self.conv_layers = []\n",
    "        for i, (num_filters, filter_size, padding, stride) in enumerate(conlayer):\n",
    "            if i == 0:\n",
    "                input_channels = input_size[-1]\n",
    "            else:\n",
    "                input_channels = conlayer[i - 1][0]  # 上一层的滤波器数量\n",
    "\n",
    "            # 初始化卷积层的权重\n",
    "            conv_weights = np.random.randn(num_filters, filter_size, filter_size, input_channels) / (filter_size * filter_size)\n",
    "            conv_bias = np.zeros(num_filters)\n",
    "            self.conv_layers.append((conv_weights, conv_bias, padding, stride))\n",
    "\n",
    "        num_fc_input = (input_size[0] // (2 ** len(conlayer))) * (input_size[1] // (2 ** len(conlayer))) * conlayer[-1][0]\n",
    "        num_fc_units, num_classes = fclayer\n",
    "\n",
    "        # 初始化全连接层的权重\n",
    "        self.fc_weights = np.random.randn(num_fc_input, num_fc_units) / num_fc_input\n",
    "        self.fc_bias = np.zeros(num_fc_units)\n",
    "        # 初始化输出层权重\n",
    "        self.output_weights = np.random.randn(num_fc_units, num_classes) / num_fc_units\n",
    "        self.output_bias = np.zeros(num_classes)\n",
    "    def ReLU(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def conv_forward(self, x, conv_weights, conv_bias, padding, stride):\n",
    "        \"\"\"\n",
    "        卷积层的前向传播\n",
    "        参数：\n",
    "        - x: 输入数据，例如图像数据\n",
    "        - conv_weights: 卷积核的权重\n",
    "        - conv_bias: 卷积核的偏置\n",
    "        - padding: 填充大小\n",
    "        - stride: 步长\n",
    "        \n",
    "        return：\n",
    "        - output: 经过卷积操作后的输出\n",
    "        \"\"\"\n",
    "        xsize, h, w, _ = x.shape[1:]\n",
    "        num_filters, _, _, _ = conv_weights.shape\n",
    "        conv_output = np.zeros(xsize, ((h - 2*padding) // stride + 1,\n",
    "                                (w - 2*padding) // stride + 1), num_filters)\n",
    "\n",
    "        # 填充0\n",
    "        padded_input = np.pad(x, ((0, 0), (padding, padding), (padding, padding), (0, 0)), mode='constant')\n",
    "\n",
    "        for k in range(xsize):\n",
    "            for i in range(0, h - 2*padding, stride):\n",
    "                for j in range(0, w - 2*padding, stride):\n",
    "                        im_region = padded_input[k, i:(i + conv_weights.shape[1] + 1), j:(j + conv_weights.shape[1] + 1),:]\n",
    "                        im_region = [im_region*num_filters]\n",
    "                        conv_output[k, i, j] = ReLU(np.sum(im_region * conv_weights, axis=(1, 2, 3)) + conv_bias)\n",
    "\n",
    "        return conv_output\n",
    "    \n",
    "    def pool_forward(self, x):\n",
    "        \"\"\"\n",
    "        池化层的前向传播\n",
    "        参数：\n",
    "        - x: 输入数据，例如经过卷积层后的输出\n",
    "        return：\n",
    "        - output: 经过池化操作后的输出\n",
    "        \"\"\"\n",
    "        xsize, h, w, num_filters = x.shape\n",
    "        pool_output = np.zeros(xsize, ((h // self.poolingLayer), (w // self.poolingLayer), num_filters))\n",
    "        \n",
    "        for k in range(xsize):\n",
    "            for f in range(num_filters):\n",
    "                for i in range(0, h, self.poolingLayer):\n",
    "                    for j in range(0, w, self.poolingLayer):\n",
    "                        pool_output[k, i // self.poolingLayer, j // self.poolingLayer, f] = np.max(x[i:i+self.poolingLayer, j:j+self.poolingLayer, f])\n",
    "\n",
    "        return pool_output\n",
    "    \n",
    "    def fc_forward(self, x, weights, bias):\n",
    "        \"\"\"\n",
    "        全连接层的前向传播\n",
    "        参数：\n",
    "        - x: 输入数据，例如经过池化层后的输出\n",
    "        - weights: 全连接层的权重\n",
    "        - bias: 全连接层的偏置\n",
    "        return：\n",
    "        - output: 经过全连接层操作后的输出\n",
    "        \"\"\"\n",
    "        self.fc_input = x.reshape((x.shape[0], -1))\n",
    "        fc_output = np.dot(self.fc_input, weights) + bias\n",
    "        return fc_output\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_values = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n",
    "        return probabilities\n",
    "    \n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        交叉熵损失函数\n",
    "        \"\"\"\n",
    "        batch_size = y_pred.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / batch_size\n",
    "        return loss\n",
    "    \n",
    "    def cross_entropy_loss_backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        交叉熵损失函数的反向传播\n",
    "        return：\n",
    "        d_L_d_out: 梯度\n",
    "        \"\"\"\n",
    "        batch_size = y_true.shape[0]\n",
    "        d_L_d_out = -y_true / (y_pred + 1e-9)\n",
    "        return d_L_d_out / batch_size\n",
    "    \n",
    "    def fc_backward(self, d_L_d_out):\n",
    "        \"\"\"\n",
    "        全连接层的反向传播\n",
    "        \"\"\"\n",
    "        d_L_d_fc_weights = np.dot(self.fc_input.T, d_L_d_out)\n",
    "        d_L_d_fc_bias = np.sum(d_L_d_out, axis=0, keepdims=True)\n",
    "        d_L_d_fc_input = np.dot(d_L_d_out, self.fc_weights.T)\n",
    "\n",
    "        self.fc_weights -= self.learn_rate * d_L_d_fc_weights\n",
    "        self.fc_bias -= self.learn_rate * d_L_d_fc_bias\n",
    "\n",
    "        return d_L_d_fc_input\n",
    "    \n",
    "    def pool_backward(self, d_L_d_pool, y):\n",
    "        \"\"\"\n",
    "        池化层的反向传播\n",
    "        \"\"\"\n",
    "        d_L_d_input = np.zeros_like(y)\n",
    "\n",
    "        for i in range(d_L_d_pool.shape[1]):\n",
    "            for j in range(d_L_d_pool.shape[2]):\n",
    "                for k in range(d_L_d_pool.shape[3]):\n",
    "                    patch = y[:, i*self.pool_size:i*self.pool_size+self.pool_size, j*self.pool_size:j*self.pool_size+self.pool_size, k]\n",
    "                    max_val = np.max(patch, axis=(1, 2))\n",
    "                    d_L_d_input[:, i*self.pool_size:i*self.pool_size+self.pool_size, j*self.pool_size:j*self.pool_size+self.pool_size, k] = (patch == max_val[:, None, None]) * d_L_d_pool[:, i, j, k][:, None, None]\n",
    "\n",
    "        return d_L_d_input\n",
    "    \n",
    "    def conv_backward(self, d_L_d_conv, x):\n",
    "        \"\"\"\n",
    "        卷积层的反向传播\n",
    "\n",
    "        \"\"\"\n",
    "        h, w, _ = x.shape[1:]\n",
    "        num_filters, _, _, _ = self.conv_layers[0][0].shape\n",
    "        d_L_d_input = np.zeros_like(x)\n",
    "        d_L_d_conv_weights = np.zeros_like(self.conv_layers[0][0])\n",
    "        d_L_d_conv_bias = np.zeros_like(self.conv_layers[0][1])\n",
    "\n",
    "        padded_input = np.pad(x, ((0, 0), (1, 1), (1, 1), (0, 0)), mode='constant')\n",
    "\n",
    "        for i in range(0, h, self.conv_layers[0][3]):\n",
    "            for j in range(0, w, self.conv_layers[0][3]):\n",
    "                im_region = padded_input[:, i:i+self.conv_layers[0][1], j:j+self.conv_layers[0][1]]\n",
    "                for f in range(num_filters):\n",
    "                    d_L_d_conv[:, i, j, f] = np.sum(d_L_d_conv[:, i:i+1, j:j+1, f][:, :, :, None] * self.conv_layers[0][0][None, :, :, :], axis=(1, 2, 3))\n",
    "                    d_L_d_conv_weights[f, :, :, :] += np.sum(im_region * (d_L_d_conv[:, i, j, f])[:, None, None, None], axis=0)\n",
    "                    d_L_d_conv_bias[f] += np.sum(d_L_d_conv[:, i, j, f], axis=0)\n",
    "\n",
    "        self.conv_layers[0][0] -= self.learn_rate * d_L_d_conv_weights\n",
    "        self.conv_layers[0][1] -= self.learn_rate * d_L_d_conv_bias\n",
    "\n",
    "        return d_L_d_input\n",
    "\n",
    "    def train(self, x_train, y_train, epochs, batch_size, learn_rate=0.1):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "        - x_train: 训练数据\n",
    "        - y_train: 训练标签\n",
    "        - epochs: 迭代次数\n",
    "        - batch_size: 批量大小\n",
    "        - learn_rate: 学习率\n",
    "        \"\"\"\n",
    "        self.learn_rate = learn_rate\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "                conv_output = x_batch\n",
    "                for conv_weights, conv_bias, padding, stride in self.conv_layers:\n",
    "                    conv_output = self.conv_forward(conv_output, conv_weights, conv_bias, padding, stride)\n",
    "                    conv_output = self.pool_forward(conv_output)\n",
    "                \n",
    "                # 全连接层输出\n",
    "                fc_output = self.fc_forward(conv_output, self.fc_weights, self.fc_bias)\n",
    "                output = self.softmax(fc_output)\n",
    "                \n",
    "                loss = self.cross_entropy_loss(output, y_batch)\n",
    "\n",
    "                d_L_d_out = self.cross_entropy_loss_backward(output, y_batch)\n",
    "                d_L_d_fc = self.fc_backward(d_L_d_out)\n",
    "                d_L_d_pool = self.pool_backward(d_L_d_fc, conv_output)\n",
    "                d_L_d_conv = self.conv_backward(d_L_d_pool, x_batch)\n",
    "\n",
    "                acc = np.mean(np.argmax(output, axis=-1) == np.argmax(y_batch, axis=-1))\n",
    "                print(f\"Epoch {epoch + 1}, Batch {i // batch_size + 1}, Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2092a02e",
   "metadata": {},
   "source": [
    "### 获取MINIST数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e006ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 获取数据集\n",
    "def read_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        f.read(4)\n",
    "        num_items = int.from_bytes(f.read(4), 'big')\n",
    "        # 读取标签数据\n",
    "        labels = [int.from_bytes(f.read(1), 'big') for _ in range(num_items)]\n",
    "    return labels\n",
    "\n",
    "\n",
    "def read_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        f.read(4)\n",
    "        num_images = int.from_bytes(f.read(4), 'big')\n",
    "        rows = int.from_bytes(f.read(4), 'big')\n",
    "        cols = int.from_bytes(f.read(4), 'big')\n",
    "        # 读取图像数据\n",
    "        images = np.zeros((num_images, rows, cols), dtype=np.uint8)\n",
    "        for i in range(num_images):\n",
    "            for row in range(rows):\n",
    "                for col in range(cols):\n",
    "                    pixel = int.from_bytes(f.read(1), 'big')\n",
    "                    images[i, row, col] = pixel\n",
    "        images = images.reshape(-1,28,28,1)\n",
    "\n",
    "    return images\n",
    "\n",
    "test_labels = read_labels('./data/MNIST/test-labels-idx1-ubyte')\n",
    "train_labels = read_labels(\"./data/MNIST/train-labels-idx1-ubyte\")\n",
    "test_images = read_images('./data/MNIST/test-images-idx3-ubyte')\n",
    "train_images = read_images('./data/MNIST/train-images-idx3-ubyte')\n",
    "\n",
    "# 归一化、one-hot编码\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "train_labels = np.eye(10)[train_labels]\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "test_labels = np.eye(10)[test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10d9c6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f3540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSize = train_images.shape[1:]\n",
    "conlSize = [(32,3,1,1), (32,3,1,1)]\n",
    "fcSize = (128, 10)\n",
    "poolSize = 2\n",
    "learning_rate = 0.1\n",
    "cnn_mnist = CNN(inputSize,conlSize,poolSize,fcSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59563e14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (3,2)  and requested shape (4,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35952\\3951691686.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcnn_mnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35952\\716359712.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_train, y_train, epochs, batch_size, learn_rate)\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[0mconv_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mconv_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_bias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                     \u001b[0mconv_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_bias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m                     \u001b[0mconv_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35952\\716359712.py\u001b[0m in \u001b[0;36mconv_forward\u001b[1;34m(self, x, conv_weights, conv_bias, padding, stride)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m# 填充0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mpadded_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'constant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpad\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32md:\\d\\python37\\lib\\site-packages\\numpy\\lib\\arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;31m# Broadcast to shape (array.ndim, 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m     \u001b[0mpad_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_as_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    744\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\d\\python37\\lib\\site-packages\\numpy\\lib\\arraypad.py\u001b[0m in \u001b[0;36m_as_pairs\u001b[1;34m(x, ndim, as_index)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;31m# Converting the array with `tolist` seems to improve performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;31m# when iterating and indexing the result (see usage in `pad`)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbroadcast_to\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32md:\\d\\python37\\lib\\site-packages\\numpy\\lib\\stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_to\u001b[1;34m(array, shape, subok)\u001b[0m\n\u001b[0;32m    409\u001b[0m            [1, 2, 3]])\n\u001b[0;32m    410\u001b[0m     \"\"\"\n\u001b[1;32m--> 411\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_broadcast_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\d\\python37\\lib\\site-packages\\numpy\\lib\\stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_to\u001b[1;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[0;32m    348\u001b[0m     it = np.nditer(\n\u001b[0;32m    349\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi_index'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'refs_ok'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'zerosize_ok'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextras\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         op_flags=['readonly'], itershape=shape, order='C')\n\u001b[0m\u001b[0;32m    351\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;31m# never really has writebackifcopy semantics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2)  and requested shape (4,2)"
     ]
    }
   ],
   "source": [
    "cnn_mnist.train(train_images,train_labels, epochs=10, batch_size=64, learn_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92fcf87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "  \n",
    "# 假设的 im_region 和 conv_weights 数组  \n",
    "im_region = np.random.rand(2, 3, 3, 1)  \n",
    "conv_weights = np.random.rand(2, 3, 3, 1)  \n",
    "  \n",
    "# 计算乘积并求和  \n",
    "result = np.sum(im_region * conv_weights, axis=(1, 2, 3))  \n",
    "  \n",
    "# 输出结果数组的形状  \n",
    "print(result.shape)  # 输出: (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c679af4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.44728344, 2.31644442])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
